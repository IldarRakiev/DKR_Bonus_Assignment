{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GIN Link Prediction - Multi-Dataset Testing\n",
        "\n",
        "Test GIN model on multiple SemanticGraph datasets from Science4Cast competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from datetime import date\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Kaggle dataset path\n",
        "DATASETS_FOLDER = \"/kaggle/input/science4cast/Science4Cast_18datasets\"\n",
        "\n",
        "# Datasets to test (first 5)\n",
        "DATASET_FILES = [\n",
        "    \"SemanticGraph_delta_1_cutoff_25_minedge_1.pkl\",\n",
        "    \"SemanticGraph_delta_1_cutoff_25_minedge_3.pkl\", \n",
        "    \"SemanticGraph_delta_1_cutoff_5_minedge_1.pkl\",\n",
        "    \"SemanticGraph_delta_3_cutoff_25_minedge_1.pkl\",\n",
        "    \"SemanticGraph_delta_3_cutoff_25_minedge_3.pkl\",\n",
        "]\n",
        "\n",
        "# Model hyperparameters\n",
        "HIDDEN_DIM = 32\n",
        "NUM_LAYERS = 2\n",
        "EDGE_HIDDEN = 32\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 1024\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# Other\n",
        "TRAIN_RATIO = 0.9\n",
        "SEED = 42\n",
        "NUM_OF_VERTICES = 64719\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Datasets: {len(DATASET_FILES)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================\n",
        "\n",
        "class GINConvLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, eps=0.0, train_eps=True):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(),\n",
        "            nn.Linear(out_dim, out_dim), nn.BatchNorm1d(out_dim))\n",
        "        self.eps = nn.Parameter(torch.tensor([eps])) if train_eps else eps\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "        agg = torch.zeros(x.size(0), x.size(1), device=x.device)\n",
        "        agg.index_add_(0, row, x[col])\n",
        "        return self.mlp((1 + self.eps) * x + agg)\n",
        "\n",
        "class GINEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=64, num_layers=3, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([GINConvLayer(in_dim if i==0 else hidden_dim, hidden_dim) \n",
        "                                     for i in range(num_layers)])\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.relu(F.dropout(x, self.dropout, self.training))\n",
        "        return x\n",
        "\n",
        "class GINLinkPredictor(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=64, num_layers=3, edge_hidden=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = GINEncoder(in_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = nn.Sequential(nn.Linear(2*hidden_dim, edge_hidden), nn.ReLU(), nn.Linear(edge_hidden, 1))\n",
        "    \n",
        "    def forward(self, x, edge_index, pairs):\n",
        "        z = self.encoder(x, edge_index)\n",
        "        return self.decoder(torch.cat([z[pairs[:,0]], z[pairs[:,1]]], dim=1)).squeeze(-1)\n",
        "\n",
        "print(\"Model defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def build_edge_index(edges, year_start):\n",
        "    cutoff = (date(year_start, 12, 31) - date(1990, 1, 1)).days\n",
        "    e = edges[edges[:, 2] < cutoff][:, :2]\n",
        "    idx = [[int(a), int(b)] for a, b in e] + [[int(b), int(a)] for a, b in e]\n",
        "    return torch.tensor(idx, dtype=torch.long).t().contiguous()\n",
        "\n",
        "def split_data(pairs, labels, ratio=0.9, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    pos, neg = np.where(labels==1)[0], np.where(labels==0)[0]\n",
        "    np.random.shuffle(pos); np.random.shuffle(neg)\n",
        "    p_tr, n_tr = int(len(pos)*ratio), int(len(neg)*ratio)\n",
        "    tr_idx = np.concatenate([pos[:p_tr], neg[:n_tr]])\n",
        "    te_idx = np.concatenate([pos[p_tr:], neg[n_tr:]])\n",
        "    np.random.shuffle(tr_idx); np.random.shuffle(te_idx)\n",
        "    return pairs[tr_idx], labels[tr_idx], pairs[te_idx], labels[te_idx]\n",
        "\n",
        "def compute_auc(pred, labels):\n",
        "    idx = np.argsort(-pred)\n",
        "    labs = labels[idx]\n",
        "    n_pos, n_neg = labs.sum(), len(labs) - labs.sum()\n",
        "    if n_pos == 0 or n_neg == 0: return 0.5\n",
        "    fp, rank_sum = 0, 0.0\n",
        "    for l in labs:\n",
        "        if l == 0: fp += 1\n",
        "        else: rank_sum += fp\n",
        "    return 1.0 - rank_sum / (n_pos * n_neg)\n",
        "\n",
        "def train_epoch(model, x, edge_idx, pairs, labels, opt, crit, bs, dev):\n",
        "    model.train()\n",
        "    perm = torch.randperm(len(pairs), device=dev)\n",
        "    pairs, labels = pairs[perm], labels[perm]\n",
        "    loss_sum, n = 0, 0\n",
        "    with torch.no_grad():\n",
        "        z = model.encoder(x, edge_idx)\n",
        "    for i in range(0, len(pairs), bs):\n",
        "        bp, bl = pairs[i:i+bs], labels[i:i+bs].float()\n",
        "        opt.zero_grad()\n",
        "        logits = model.decoder(torch.cat([z[bp[:,0]], z[bp[:,1]]], dim=1)).squeeze(-1)\n",
        "        loss = crit(logits, bl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_sum += loss.item(); n += 1\n",
        "    return loss_sum / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, x, edge_idx, pairs, labels, bs, dev):\n",
        "    model.eval()\n",
        "    z = model.encoder(x, edge_idx)\n",
        "    logits = []\n",
        "    for i in range(0, len(pairs), bs):\n",
        "        bp = pairs[i:i+bs]\n",
        "        logits.append(model.decoder(torch.cat([z[bp[:,0]], z[bp[:,1]]], dim=1)).squeeze(-1).cpu())\n",
        "    return compute_auc(torch.cat(logits).numpy(), labels.cpu().numpy())\n",
        "\n",
        "print(\"Functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TRAIN ON MULTIPLE DATASETS\n",
        "# ============================================\n",
        "\n",
        "set_seed(SEED)\n",
        "results = []\n",
        "\n",
        "for dataset_file in DATASET_FILES:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"DATASET: {dataset_file}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    path = os.path.join(DATASETS_FOLDER, dataset_file)\n",
        "    \n",
        "    try:\n",
        "        # Load\n",
        "        data = load_data(path)\n",
        "        edges, pairs, labels, year, delta, cutoff, min_e = data\n",
        "        print(f\"  delta={delta}, cutoff={cutoff}, min_edges={min_e}\")\n",
        "        print(f\"  pairs={len(pairs)}, positive={labels.sum()}\")\n",
        "        \n",
        "        # Build graph\n",
        "        edge_idx = build_edge_index(edges, year).to(DEVICE)\n",
        "        print(f\"  edges={edge_idx.size(1)}\")\n",
        "        \n",
        "        # Split\n",
        "        tr_p, tr_l, te_p, te_l = split_data(pairs, labels, TRAIN_RATIO, SEED)\n",
        "        tr_p = torch.tensor(tr_p, dtype=torch.long, device=DEVICE)\n",
        "        tr_l = torch.tensor(tr_l, dtype=torch.long, device=DEVICE)\n",
        "        te_p = torch.tensor(te_p, dtype=torch.long, device=DEVICE)\n",
        "        te_l = torch.tensor(te_l, dtype=torch.long, device=DEVICE)\n",
        "        \n",
        "        # Features\n",
        "        x = torch.ones(NUM_OF_VERTICES, 1, device=DEVICE)\n",
        "        \n",
        "        # Model\n",
        "        model = GINLinkPredictor(1, HIDDEN_DIM, NUM_LAYERS, EDGE_HIDDEN, DROPOUT).to(DEVICE)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        crit = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "        # Train\n",
        "        best_auc = 0\n",
        "        t0 = time.time()\n",
        "        for ep in range(1, EPOCHS+1):\n",
        "            loss = train_epoch(model, x, edge_idx, tr_p, tr_l, opt, crit, BATCH_SIZE, DEVICE)\n",
        "            auc = evaluate(model, x, edge_idx, te_p, te_l, BATCH_SIZE, DEVICE)\n",
        "            if auc > best_auc: best_auc = auc\n",
        "            if ep % 5 == 0 or ep == 1:\n",
        "                print(f\"  Epoch {ep:2d}: loss={loss:.4f}, AUC={auc:.4f}\")\n",
        "        \n",
        "        final_auc = evaluate(model, x, edge_idx, te_p, te_l, BATCH_SIZE, DEVICE)\n",
        "        elapsed = time.time() - t0\n",
        "        \n",
        "        results.append({\n",
        "            'dataset': dataset_file, 'delta': delta, 'cutoff': cutoff,\n",
        "            'min_edges': min_e, 'best_auc': best_auc, 'final_auc': final_auc, 'time': elapsed\n",
        "        })\n",
        "        print(f\"\\n  ✓ FINAL AUC: {final_auc:.4f} (best: {best_auc:.4f}) in {elapsed:.1f}s\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, opt, tr_p, tr_l, te_p, te_l, edge_idx, x\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ ERROR: {e}\")\n",
        "        results.append({'dataset': dataset_file, 'delta': None, 'cutoff': None,\n",
        "                       'min_edges': None, 'best_auc': None, 'final_auc': None, 'time': None})\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DONE!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# RESULTS SUMMARY\n",
        "# ============================================\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Stats\n",
        "valid = [r['final_auc'] for r in results if r['final_auc']]\n",
        "if valid:\n",
        "    print(f\"\\nAverage AUC: {np.mean(valid):.4f}\")\n",
        "    print(f\"Min AUC: {np.min(valid):.4f}\")\n",
        "    print(f\"Max AUC: {np.max(valid):.4f}\")\n",
        "\n",
        "# Plot\n",
        "if len(valid) > 1:\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    names = [r['dataset'].replace('SemanticGraph_', '').replace('.pkl', '') for r in results if r['final_auc']]\n",
        "    ax.bar(range(len(valid)), valid, color='steelblue')\n",
        "    ax.set_xticks(range(len(valid)))\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "    ax.set_ylabel('AUC')\n",
        "    ax.set_title('GIN Link Prediction Results')\n",
        "    ax.axhline(0.5, color='red', linestyle='--', label='Random')\n",
        "    ax.set_ylim(0.4, 1.0)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
